<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <title>Multi-Object Tracking with Hallucinated and Unlabeled Videos</title>

    <!-- Bootstrap core CSS -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="../index.css" rel="stylesheet">

    <!-- Font Awesome for Icons -->
    <link rel="stylesheet" href="../font-awesome-4.6.3/css/font-awesome.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>

</head>

<body>
    <section id="intro" class="container content-section text-center">
        <table class="container content-section text-left"></table>
        <div class="row">
            <table>
                <tr>
                    <h2>Multi-Object Tracking with Hallucinated and Unlabeled Videos</h2>
                </tr>
                <table align=center width=720px style="font-size:18px;">
                    <tr>
                        <br>
                        <td width=180px>
                            <span>Daniel McKee</span>
                        </td>
                        <td width=180px>
                            <span>Bing Shuai</span>
                        </td>
                        <td width=180px>
                            <span>Andrew Berneshawi</span>
                        </td>
                        <td width=180px>
                            <span>Manchen Wang</span>
                        </td>
                    </tr>
                </table>
                <table align=center width=540px style="font-size:18px;">
                    <tr>
                        <td width=180px>
                            <span>Davide Modolo</span>
                        </td>
                        <td width=180px>
                            <span>Svetlana Lazebnik</span>
                        </td>
                        <td width=180px>
                            <span>Joseph Tighe</span>
                        </td>
                    </tr>
                </table>
                <tr>
                    <br>
                    <span style="font-size:16px">
                    University of Illinois at Urbana-Champaign<br> Amazon Web Services<br>
                </span>
                </tr>
            </table>
            <table>
                <tr>
                    <br>
                    <span style="font-size:18px">Presented at CVPR 2021 Workshop on <a href="https://sites.google.com/view/luv2021" target="_blank">Learning from Unlabeled Videos</a><span>
                </tr>
            </table>
            <table align=center width=300px style="font-size:20px;">
                <tr>
                    <br>
                    <td width=150px>
                        <span><a href='https://arxiv.org/abs/2108.08836' target="_blank"> [arXiv]</a></span>
                    </td>
                    <td width=150px>
                        <span><a href='https://www.youtube.com/watch?v=Vayu2_s58nU' target="_blank"> [Video]</a></span>
                    </td>
                </tr>
            </table>
            <table>
                <tr>
                    <img width="500px" id="teaser" src="../images/weakly_supervised_teaser.png">
                    <br>
                </tr>
            </table>
        </div>
    </section>


    <section id="about" class="container content-section text-center">
        <div class="row">
            <h3>Abstract</h3>
            <div class="text-left">
                <p>In this paper, we explore learning end-to-end deep neural trackers without tracking annotations. This is important as large-scale training data is essential for training deep neural trackers while tracking annotations are expensive to
                    acquire. In place of tracking annotations, we first hallucinate videos from images with bounding box annotations using zoom-in/out motion transformations to obtain free tracking labels. We add video simulation augmentations to create
                    a diverse tracking dataset, albeit with simple motion. Next, to tackle harder tracking cases, we mine hard examples across an unlabeled pool of real videos with a tracker trained on our hallucinated video data. For hard example mining,
                    we propose an optimization-based connecting process to first identify and then rectify hard examples from the pool of unlabeled videos. Finally, we train our tracker jointly on hallucinated data and mined hard video examples. Our weakly
                    supervised tracker achieves state-of-the-art performance on the MOT17 and TAO-person datasets. On MOT17, we further demonstrate that the combination of our self-generated data and the existing manually-annotated data leads to additional
                    improvements.
                </p>

            </div>
    </section>
</body>

</html>